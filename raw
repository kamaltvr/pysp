import findspark findspark.init() import matplatlib.pyplot as pit ematblotiib inline import seaborn as sns from sklearn.model_selection import train_test_split from sklearn preprocessing import LabelBinarizer, LabelEncoder from sklearn.metrics import confusion matrix, accuracy score import tensorflow as tf. from tensorflow import keras from tensorflow. keras.layers import Dense, Input from tensorflow.keras.optimizers import Adam from tensorflow.keras models import Model from tensorflow. keras import utils as np utils from keras.utils.np utils import to categorical
9 10 11 12
14 15 16 17 18
from pyspark.context import SparkContext from pyspark.sgl.session import SparkSession sc = SparkContext('local') spark = SparkSession (sc)
20
22
filepath = 'index_data_v1.csv'
25 26
33
data df = spark.read.option ("header","true").csy (filepath) data df = data df. filter (data df. Details.isNotNull())
data df = data df. filter (data df. Category.isNotNull()) 27 from pyspark.ml. feature import RegexTokenizer, StopWordsRemover, CountVectorizer 28 from pyspark.ml.classification import LogisticRegression 29 # regular expression tokenizer 30 regexTokenizer = RegexTokenizer (inputCol="Details", outputcol="words", pattern="\\w") 31 # stop words 32 add stopwords = ["http","https", "amp", "rt","t","C", "the"]
stopwordsRemover = StopwordsRemover(inputCol="words", outputCol="filtered").setStopWords (add_stopwords) # bag of words count
countVectors = CountVectorizer (inputCol="filtered", outputCol="features", vocabSize=10000, minDF=5) 36 from pyspark.ml import Pipeline 37 from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
label_stringIdx = StringIndexer (inputCol = "Category", outputcol = "label") 39 pipeline = Pipeline (stages=[regexTokenizer, stopwordsRemover, countVectors, label stringIdx]) 40 # Fit the pipeline to training documents. 41 pipelineFit = pipeline.fit (data df) 42 dataset = pipelineFit.transform (data df) 43 dataset.show (5) 44 # set seed for reproducibility


44 # set seed for reproducibility 45 (trainingData, testData) = dataset.randomSplit([0.7, 0.3), seed = 100) 46 print ("Training Dataset Count: " + str(trainingData.count())) 47 print ("Test Dataset Count: " + str (testData.count()))
49 #(trainDF, testDF) = data df.randomSplit((0.75, 0.25), seed=100) 50 lr = LogisticRegression (maxIter=20, regParam=0.3, elasticNetParam=0) 51 lrModel = lr. fit (trainingData)
predictions = lrModel.transform(testData) 53 from pyspark.ml.evaluation import MulticlassclassificationEvaluator
evaluator = MulticlassclassificationEvaluator (predictionCol="prediction") 55 evaluator.evaluate (predictions)
54 eta
56
58
57 TF-IDF
from pyspark.ml.feature import HashingTF, IDF 59 hashingTF = HashingTF (inputCol="filtered", outputCol="rawFeatures", numFeatures=10000) 60 idf = IDF(inputCol="rawFeatures", outputcol="features", minDocFreq=5) #minDocFreq: remove sparse terms 61 pipeline = Pipeline (stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx]) 62 pipeline Fit = pipeline.fit (data df) 63 dataset = pipelineFit.transform (data_df) 64 (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100) 65 lr = LogisticRegression (maxIter=20, regParam=0.3, elasticNetParam=0) 66 lrModel = lr.fit (trainingData) 67 predictions = lrModel.transform(testData) 68 69 evaluator = MulticlassClassificationEvaluator (predictionCol="prediction") 70 evaluator.evaluate (predictions) 71 pipeline = Pipeline (stages=[regexTokenizer, stopwordsRemover, countVectors, label stringIdx])
pipelineFit = pipeline.fit (data_df) 73 dataset = pipelineFit.transform (data_df) 74 (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100) 75 1r = LogisticRegression (maxIter=20, regParam=0.3, elasticNetParam=0) 76 from pyspark.ml. tuning import ParamGridBuilder, CrossValidator 77 # Create ParamGrid for Cross Validation 78 paramGrid = (ParamGridBuilder()
.addGrid (lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter
.addGrid (lr.elasticNetParam, (0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0) 81 #
.addGrid (model.maxIter, [10, 20, 50]) #Number of iterations 82 #
.addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features .build())


Create 5-fold CrossValidator cv = CrossValidator (estimatorul,
estimator ParamMaps=paramGrid, evaluator-evaluator,
numFolds=5) CVModel = cv.fit(trainingData)
predictions = cvModel.transform(testData) # Evaluate best model evaluator = MulticlassclassificationEvaluator (predictioncol="prediction") evaluator.evaluate (predictions)
#Cross validation pipeline = Pipeline (stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx]) pipelineFit = pipeline.fit (data df) dataset = pipelineFit.transform (data df) (trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100) lr = LogisticRegression (maxIter=20, reg Param=0.3, elasticNetParam=0) from pyspark.ml.tuning import ParamGridBuilder, CrossValidator # Create ParamGrid for Cross Validation paramGrid = (ParamGridBuilder()
.addGrid (lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter .addGrid (lr.elasticNetParam, (0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0) .addGrid (model.maxIter, [10, 20, 50]) #Number of iterations .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features
.build()) # Create 5-fold CrossValidator cv = CrossValidator (estimator=14,1
estimator ParamMaps=paramGrid, evaluator=evaluator,
numFolds=5) CVModel = cv.fit (trainingData)
SIE
SIE
predictions = cvModel.transform(testData) # Evaluate best model evaluator = MulticlassclassificationEvaluator (predictioncol="prediction") evaluator.evaluate (predictions)


102 from pyspark.ml. tuning import ParamGridBuilder, CrossValidator 103 # Create ParamGrid for Cross Validation 104 paramGrid = (ParamGridBuilder() 105
.addGrid (lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter 106
.addGrid (lr.elasticNetParam, (0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0) 107
.addGrid (model.maxIter, [10, 20, 50]) #Number of iterations 108 #
.addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features 109
.build()) 110 # Create 5-fold CrossValidator 111 cv = CrossValidator (estimator=lr, 112
estimator ParamMaps=paramGrid, 113
evaluator-evaluator,
numFolds=5) 115 CVModel = cv.fit (trainingData)
114
116
123
117 predictions = cvModel.transform(testData) 118 # Evaluate best model 119 evaluator = MulticlassclassificationEvaluator (predictioncol="prediction") 120 evaluator.evaluate (predictions) 121 122
#Naive baves 124 from pyspark.ml.classification import NaiveBayes 125 nb = NaiveBayes (smoothing=1) 126 model = nb. fit (trainingData) 127 predictions = model.transform(testData) 128 129 evaluator = MulticlassclassificationEvaluator (predictioncol="prediction") 130 evaluator evaluate (predictions) 131 132 #Random Forest 133 from pyspark.ml.classification import RandomForestclassifier 134 rf = Random Forestclassifier (labelcol="label", 135
featuresCol="features", 136
numTrees = 100, 137
maxDepth = 4, 138
maxBins = 32) 139 # Train model with Training Data 140 rfModel = rf.fit(trainingData) 141 predictions = rfModel.transform (testData) 142 143 evaluator = MulticlassclassificationEvaluator (predictioncol="prediction") 144 evaluator.evaluate (predictions)
